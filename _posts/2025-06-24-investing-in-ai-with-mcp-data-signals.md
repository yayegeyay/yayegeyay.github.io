---
layout: single
title: "Investing in AI: Modular Context Protocols & Data Signals"
date: 2025-05-30 09:30:00 +0000
categories: strategy architecture
---

> tl;dr: Before you fine-tune anything, make sure you can **see** what users are actually doing.

VPs keep asking me how to choose the *one* AI bet that will impress their board this quarter. Wrong question. The right move is to install a thin layer—call it a **Modular Context Protocol (MCP)**—that lets every idea ship as a simple chatbot first. Usage data then tells you which workflows deserve agents, and which agents deserve full-blown apps.

### Three levels of investment

1. **Chat discovery** – 48-hour prototype. Logs show real user intent.
2. **Agent workflow** – Automate the high-volume patterns that surface.
3. **Custom UI** – Only when ARR per workflow justifies pixel-perfect UX.

At CloudSale.ai we found that 40 % of chat queries were *prospect research*. That single metric justified an autonomous research agent, which now runs 24/7 and feeds the human BDR only qualified leads.

### Data first, models later

Your eval suite is only as good as the prod logs feeding it. Treat logs, evaluation datasets, and feedback loops as a *shared asset*—just like a central data warehouse. Every team that forks their own metrics is burning future engineering time.

If your CTO's roadmap still lists "fine-tune proprietary LLM" before "standardise logs," reverse them.

**Framework to steal**

`value = volume × success_rate × revenue_per_interaction`

Rank workflows weekly, invest accordingly. That's the closest thing to an AI crystal ball I've found. 